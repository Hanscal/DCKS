# -*- coding: utf-8 -*-

"""
@Time    : 2022/6/9 11:11 下午
@Author  : hcai
@Email   : hua.cai@unidt.com
"""

import logging
import warnings
from typing import Optional, Iterable

import torch
from torch import nn
from torch.nn import CrossEntropyLoss
import torch.nn.functional as F
from transformers import BartConfig

from src.model.model_utils import GenerationMixinOld
from src.model.model import PretrainedBartModel
from src.model.model import BartModel, EncoderLayer
from src.model.emo_encoder import make_encoder
from src.model.model import _make_linear_from_emb, _reorder_buffer, shift_tokens_right
from config import EMO_MAP
from config import args as config_args

logger = logging.getLogger(__name__)

# @add_start_docstrings(
#     "The BART Model with a language modeling head. Can be used for summarization.", BART_START_DOCSTRING
# )
class BartForConditionalGeneration(PretrainedBartModel, GenerationMixinOld):
    base_model_prefix = "model"
    authorized_missing_keys = [r"final_logits_bias", r"encoder\.version", r"decoder\.version"]

    def __init__(self, config: BartConfig):
        super().__init__(config)
        base_model = BartModel(config)
        self.model = base_model
        self.register_buffer("final_logits_bias", torch.zeros((1, self.model.shared.num_embeddings)))

        # knowl和ctx token级别融合
        self.emo_encoder = make_encoder(emb_dim=config.d_model, hidden_dim=config.d_model)
        self.emo_lin = nn.Linear(config.d_model, len(EMO_MAP), bias=False)  # 分类总数
        self.fc = nn.Linear(128, len(EMO_MAP), bias=False)

    def make_encoder(self, config:BartConfig):
        return EncoderLayer(config=config)


    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:
        old_num_tokens = self.model.shared.num_embeddings
        new_embeddings = super().resize_token_embeddings(new_num_tokens)
        self.model.shared = new_embeddings
        self._resize_final_logits_bias(new_num_tokens, old_num_tokens)
        return new_embeddings

    def _resize_final_logits_bias(self, new_num_tokens: int, old_num_tokens: int) -> None:
        if new_num_tokens <= old_num_tokens:
            new_bias = self.final_logits_bias[:, :new_num_tokens]
        else:
            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)
            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)
        self.register_buffer("final_logits_bias", new_bias)

    def modify_inp_and_tgt(self, tgt_ids, pad_token_id, eos_token_id):
        prev_output_tokens = tgt_ids.clone()
        index_of_eos = (tgt_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)
        prev_output_tokens[:, 0] = tgt_ids.gather(1, index_of_eos).squeeze()
        prev_output_tokens[:, 1:] = tgt_ids[:, :-1]

        return prev_output_tokens, tgt_ids

    def modify_tgt_mask(self, decoder_attention_mask):
        # return decoder_attention_mask[:, :-1]
        return decoder_attention_mask


    def prepare_inputs_for_generation(
            self, decoder_input_ids, past, attention_mask, use_cache, encoder_outputs, **kwargs
    ):
        return {
            "input_ids": None,  # encoder_outputs is defined. input_ids not needed
            "encoder_outputs": encoder_outputs,
            "past_key_values": past,
            "decoder_input_ids": decoder_input_ids,
            "attention_mask": attention_mask,
            "use_cache": use_cache,  # change this to avoid caching (presumably for debugging)
        }

    def adjust_logits_during_generation(self, logits, cur_len, max_length):
        if cur_len == 1:
            self._force_token_ids_generation(logits, self.config.bos_token_id)
        elif cur_len == max_length - 1 and self.config.eos_token_id is not None:
            self._force_token_ids_generation(logits, self.config.eos_token_id)
        return logits

    def _force_token_ids_generation(self, scores, token_id) -> None:
        """force one of token_ids to be generated by setting prob of all other tokens to 0 (logprob=-float("inf"))"""
        scores[:, [x for x in range(self.config.vocab_size) if x != token_id]] = -float("inf")

    @staticmethod
    def _reorder_cache(past, beam_idx):
        reordered_past = []
        for layer_past in past:
            # get the correct batch idx from decoder layer's batch dim for cross and self-attn
            layer_past_new = {
                attn_key: _reorder_buffer(attn_cache, beam_idx) for attn_key, attn_cache in layer_past.items()
            }
            reordered_past.append(layer_past_new)
        return reordered_past

    def get_encoder(self):
        return self.model.encoder, self.model.knowl_encoder

    def get_output_embeddings(self):
        return _make_linear_from_emb(self.model.shared)  # make it on the fly

    def forward(
            self,
            input_ids,
            attention_mask=None,
            encoder_outputs=None,
            decoder_input_ids=None,
            decoder_attention_mask=None,
            past_key_values=None,
            labels=None,
            emo_labels=None,
            use_cache=None,
            output_attentions=None,
            output_hidden_states=None,
            return_dict=None,
            reduction='mean',
            **unused,
    ):
        if "lm_labels" in unused:
            warnings.warn(
                "The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.",
                FutureWarning,
            )
            labels = unused.pop("lm_labels")
        if "decoder_cached_states" in unused:
            warnings.warn(
                "The `decoder_cached_states` argument is deprecated and will be removed in a future version, use `past_key_values` instead.",
                FutureWarning,
            )
            past_key_values = unused.pop("decoder_cached_states")
        if "decoder_past_key_values" in unused:
            warnings.warn(
                "The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.",
                FutureWarning,
            )
            past_key_values = unused.pop("decoder_past_key_values")
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        decoder_attention_mask = None
        if labels is not None:
            use_cache = False
            if decoder_input_ids is None:
                decoder_input_ids = shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)


        outputs,name_knowl = self.model(
            input_ids,
            emo_labels=emo_labels,
            emo_ln = self.emo_lin,
            attention_mask=attention_mask,
            decoder_input_ids=decoder_input_ids,
            encoder_outputs=encoder_outputs,
            decoder_attention_mask=decoder_attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        # logger.info(f"{outputs[0].shape}")
        # todo output的Encoder—last-hidden-state包含ctx和kno的，这里可以做emo—label预测
        lm_logits = F.linear(outputs[0], self.model.shared.weight, bias=self.final_logits_bias)

        masked_lm_loss = None
        if labels is not None:
            loss_fct = CrossEntropyLoss(reduction=reduction)
            # TODO(SS): do we need to ignore pad tokens in labels?
            # logger.info(f"{lm_logits.shape}, {labels.shape}")
            masked_lm_loss = loss_fct(lm_logits.reshape(-1, self.config.vocab_size), labels.reshape(-1))

        # 对emo_label多任务联合学习
        emo_loss = None
        emo_logits = None
        # print("emo_labels",emo_labels)
        if emo_labels is not None:
            # outputs.encoder_last_hidden_state 包含ctx，kno的embedding
            ctx_emb = outputs.encoder_last_hidden_state[0]
            # print('ctx_emb',ctx_emb.shape)
            bsz, seq_length = outputs.encoder_last_hidden_state[0].shape[0:2]
            knowl_emb = outputs.encoder_last_hidden_state[1]
            bsz_mul_num_kno, kno_len, d_model = knowl_emb.shape
            num_kno = bsz_mul_num_kno // bsz
            # (bsz, num_kno, kno_len, d_model)
            knowl = knowl_emb.reshape(bsz, num_kno, kno_len, d_model)
            cls_knowl = knowl[:, :, 0, :]  # 取knowl的第一个cls作为其表征
            emo_cls = torch.mean(cls_knowl, dim=1, keepdim=True)  # bsz*1*dim
            # 消融实验woKnowledge,只有ctx
            if config_args.woKnowledge:
                emo_logits = self.emo_lin(ctx_emb[:, 0])

            # 消融实验woContext，只有knowledge
            elif config_args.woContext:
                emo_concat = emo_cls.expand(-1, seq_length, -1)
                emo_ref_ctx = self.emo_encoder(emo_concat, attention_mask[0].eq(1))
                emo_logits = self.emo_lin(emo_ref_ctx[:, 0])
            # knowledge和context都包含
            else:
                emo_concat = torch.add(ctx_emb, emo_cls.expand(-1, seq_length, -1))
                # 对ctx进行mask就行,注意输入的是padding mask，需要对attention mask进行反转
                # 进行refine，即进行token level级别融合
                emo_ref_ctx = self.emo_encoder(emo_concat, attention_mask[0].eq(1))
                emo_logits = self.emo_lin(emo_ref_ctx[:, 0])
            if emo_labels.dim() == 2:
                emo_labels = emo_labels.squeeze(1)
            emo_loss = nn.CrossEntropyLoss()(emo_logits, emo_labels)

        if not return_dict:
            output = (lm_logits,) + outputs[1:]
            output = ((masked_lm_loss,) + output) if masked_lm_loss is not None else output
            output = ((emo_loss,) + output) if emo_loss is not None else output
            return output

        return {
            "mlm_loss":masked_lm_loss, "emo_loss": emo_loss,
            "logits": lm_logits, "emo_logits": emo_logits,
            "past_key_values":outputs.past_key_values,
            "decoder_hidden_states":outputs.decoder_hidden_states,
            "decoder_attentions":outputs.decoder_attentions,
            "encoder_last_hidden_state":outputs.encoder_last_hidden_state,
            "encoder_hidden_states":outputs.encoder_hidden_states,
            "encoder_attentions":outputs.encoder_attentions,
            "name_knowl":name_knowl
        }


    @torch.no_grad()
    def generate(
            self,
            input_ids: Optional[torch.LongTensor] = None,
            max_length: Optional[int] = None,
            min_length: Optional[int] = None,
            do_sample: Optional[bool] = None,
            early_stopping: Optional[bool] = None,
            num_beams: Optional[int] = None,
            temperature: Optional[float] = None,
            top_k: Optional[int] = None,
            top_p: Optional[float] = None,
            repetition_penalty: Optional[float] = None,
            bad_words_ids: Optional[Iterable[int]] = None,
            bos_token_id: Optional[int] = None,
            pad_token_id: Optional[int] = None,
            eos_token_id: Optional[int] = None,
            length_penalty: Optional[float] = None,
            no_repeat_ngram_size: Optional[int] = None,
            num_return_sequences: Optional[int] = None,
            attention_mask: Optional[torch.LongTensor] = None,
            decoder_start_token_id: Optional[int] = None,
            use_cache: Optional[bool] = None,
            **model_kwargs
    ) -> torch.LongTensor:
        # We cannot generate if the model does not have a LM head
        if self.get_output_embeddings() is None:
            raise AttributeError(
                "You tried to generate sequences with a model that does not have a LM Head."
                "Please use another model class (e.g. `OpenAIGPTLMHeadModel`, `XLNetLMHeadModel`, `GPT2LMHeadModel`, `CTRLLMHeadModel`, `T5WithLMHeadModel`, `TransfoXLLMHeadModel`, `XLMWithLMHeadModel`, `BartForConditionalGeneration` )"
            )

        max_length = max_length if max_length is not None else self.config.max_length
        min_length = min_length if min_length is not None else self.config.min_length
        do_sample = do_sample if do_sample is not None else self.config.do_sample
        early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping
        use_cache = use_cache if use_cache is not None else self.config.use_cache
        num_beams = num_beams if num_beams is not None else self.config.num_beams
        temperature = temperature if temperature is not None else self.config.temperature
        top_k = top_k if top_k is not None else self.config.top_k
        top_p = top_p if top_p is not None else self.config.top_p
        repetition_penalty = repetition_penalty if repetition_penalty is not None else self.config.repetition_penalty
        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id
        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id
        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id
        length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty
        no_repeat_ngram_size = (
            no_repeat_ngram_size if no_repeat_ngram_size is not None else self.config.no_repeat_ngram_size
        )
        bad_words_ids = bad_words_ids if bad_words_ids is not None else self.config.bad_words_ids
        num_return_sequences = (
            num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences
        )
        decoder_start_token_id = (
            decoder_start_token_id if decoder_start_token_id is not None else self.config.decoder_start_token_id
        )

        if input_ids is not None:
            batch_size = input_ids[0].shape[0]  # overriden by the input batch_size
        else:
            batch_size = 1

        assert isinstance(max_length, int) and max_length > 0, "`max_length` should be a strictly positive integer."
        assert isinstance(min_length, int) and min_length >= 0, "`min_length` should be a positive integer."
        assert isinstance(do_sample, bool), "`do_sample` should be a boolean."
        assert isinstance(early_stopping, bool), "`early_stopping` should be a boolean."
        assert isinstance(use_cache, bool), "`use_cache` should be a boolean."
        assert isinstance(num_beams, int) and num_beams > 0, "`num_beams` should be a strictly positive integer."
        assert temperature > 0, "`temperature` should be strictly positive."
        assert isinstance(top_k, int) and top_k >= 0, "`top_k` should be a positive integer."
        assert 0 <= top_p <= 1, "`top_p` should be between 0 and 1."
        assert repetition_penalty >= 1.0, "`repetition_penalty` should be >= 1."
        assert input_ids is not None or (isinstance(bos_token_id, int) and bos_token_id >= 0), "If input_ids is not defined, `bos_token_id` should be a positive integer."
        assert pad_token_id is None or (isinstance(pad_token_id, int) and (pad_token_id >= 0)), "`pad_token_id` should be a positive integer."
        assert (eos_token_id is None) or (isinstance(eos_token_id, int) and (eos_token_id >= 0)), "`eos_token_id` should be a positive integer."
        assert length_penalty > 0, "`length_penalty` should be strictly positive."
        assert (isinstance(no_repeat_ngram_size, int) and no_repeat_ngram_size >= 0), "`no_repeat_ngram_size` should be a positive integer."
        assert (isinstance(num_return_sequences, int) and num_return_sequences > 0), "`num_return_sequences` should be a strictly positive integer."
        assert (bad_words_ids is None or isinstance(bad_words_ids, list) and isinstance(bad_words_ids[0], list)), "`bad_words_ids` is either `None` or a list of lists of tokens that should not be generated"

        assert input_ids is not None
        assert len(input_ids) == 2
        assert input_ids[0].dim() == 2
        assert input_ids[1].dim() == 3
        # not allow to duplicate outputs when greedy decoding
        if do_sample is False:
            if num_beams == 1:
                # no_beam_search greedy generation conditions
                assert (num_return_sequences == 1), "Greedy decoding will always produce the same output for num_beams == 1 and num_return_sequences > 1. Please set num_return_sequences = 1"

            else:
                # beam_search greedy generation conditions
                assert (num_beams >= num_return_sequences), "Greedy beam search decoding cannot return more sequences than it has beams. Please set num_beams >= num_return_sequences"

        # create attention mask if necessary
        # TODO (PVP): this should later be handled by the forward fn() in each model in the future see PR 3140
        if (attention_mask is None) and (pad_token_id is not None) and (pad_token_id in input_ids[0]):
            attention_mask = [input_ids[0].ne(pad_token_id).long(), input_ids[1].ne(pad_token_id).long()]
        elif attention_mask is None:
            attention_mask = [input_ids[0].new_ones(input_ids[0].shape), input_ids[1].new_ones(input_ids[1].shape)]

        # attention_mask is created
        if pad_token_id is None and eos_token_id is not None:
            logger.warning("Setting `pad_token_id` to {} (first `eos_token_id`) to generate sequence".format(eos_token_id))
            pad_token_id = eos_token_id

        # current position and vocab size
        if hasattr(self.config, "vocab_size"):
            vocab_size = self.config.vocab_size
        elif (self.config.is_encoder_decoder and hasattr(self.config, "decoder") and hasattr(self.config.decoder, "vocab_size")):
            vocab_size = self.config.decoder.vocab_size

        # set effective batch size and effective batch multiplier according to do_sample
        if do_sample:
            effective_batch_size = batch_size * num_return_sequences
            effective_batch_mult = num_return_sequences
        else:
            effective_batch_size = batch_size
            effective_batch_mult = 1

        if self.config.is_encoder_decoder:
            if decoder_start_token_id is None:
                # see if BOS token can be used for decoder_start_token_id
                if bos_token_id is not None:
                    decoder_start_token_id = bos_token_id
                elif hasattr(self.config, "decoder") and hasattr(self.config.decoder, "bos_token_id"):
                    decoder_start_token_id = self.config.decoder.bos_token_id
                else:
                    raise ValueError(
                        "decoder_start_token_id or bos_token_id has to be defined for encoder-decoder generation"
                    )

            assert hasattr(self, "get_encoder"), "{} should have a 'get_encoder' function defined".format(self)
            assert callable(self.get_encoder), "{} should be a method".format(self.get_encoder)

            # get encoder and store encoder outputs
            src_encoder, knowl_encoder = self.get_encoder()
            max_num_knowl, max_knowl_len = input_ids[1].shape[-2:]
            encoder_outputs = [
                src_encoder(input_ids[0], attention_mask=attention_mask[0], return_dict=True),
                knowl_encoder(input_ids[1].reshape(-1, max_knowl_len), attention_mask=attention_mask[1].reshape(-1, max_knowl_len), return_dict=True)
            ]

            encoder_outputs[1]["last_hidden_state"] = self.model.add_knowl_pos_inferr(
                encoder_outputs[1]["last_hidden_state"],
                max_num_knowl,encoder_outputs[0]["last_hidden_state"],
                emo_ln=self.emo_lin,know_single_bundle=None
            )

        # Expand input ids if num_beams > 1 or num_return_sequences > 1
        if num_return_sequences > 1 or num_beams > 1:
            src_ids, kno_ids = input_ids
            src_attn_mask, kno_attn_mask = attention_mask
            src_ids_len = src_ids.shape[-1]
            num_know, knowl_ids_len = kno_ids.shape[-2:]

            src_ids = src_ids.unsqueeze(1).expand(batch_size, effective_batch_mult * num_beams, src_ids_len)
            kno_ids = kno_ids.unsqueeze(1).expand(batch_size, effective_batch_mult * num_beams, num_know, knowl_ids_len)

            src_attn_mask = src_attn_mask.unsqueeze(1).expand(batch_size, effective_batch_mult * num_beams, src_ids_len)
            kno_attn_mask = kno_attn_mask.unsqueeze(1).expand(batch_size, effective_batch_mult * num_beams, num_know, knowl_ids_len)

            # shape: (batch_size * num_return_sequences * num_beams, cur_len)
            src_ids = src_ids.contiguous().view(effective_batch_size * num_beams, src_ids_len)
            # shape: (batch_size * num_return_sequences * num_beams * num_know, cur_len)
            kno_ids = kno_ids.contiguous().view(effective_batch_size * num_beams * num_know, knowl_ids_len)
            # shape: (batch_size * num_return_sequences * num_beams, cur_len)
            src_attn_mask = src_attn_mask.contiguous().view(effective_batch_size * num_beams, src_ids_len)
            # shape: (batch_size * num_return_sequences * num_beams * num_know, cur_len)
            kno_attn_mask = kno_attn_mask.contiguous().view(effective_batch_size * num_beams, num_know, knowl_ids_len)
            input_ids = (src_ids, kno_ids)
            attention_mask = (src_attn_mask, kno_attn_mask)
            # print("generate:", attention_mask[0].shape, attention_mask[1].shape)

        if self.config.is_encoder_decoder:
            # create empty decoder_input_ids
            input_ids = torch.full((effective_batch_size * num_beams, 1),decoder_start_token_id,dtype=torch.long,device=next(self.parameters()).device,)
            cur_len = 1

            assert (batch_size == encoder_outputs[0].last_hidden_state.shape[0]), \
                f"expected encoder_outputs.last_hidden_state to have 1st dimension bs={batch_size}, got {encoder_outputs[0].last_hidden_state.shape[0]} "

            # expand batch_idx to assign correct encoder output for expanded input_ids (due to num_beams > 1 and num_return_sequences > 1)
            expanded_batch_idxs = (
                torch.arange(batch_size)
                    .view(-1, 1)
                    .repeat(1, num_beams * effective_batch_mult)
                    .view(-1)
                    .to(input_ids[0].device)
            )
            # print(expanded_batch_idxs.shape)
            hidden_size = encoder_outputs[1].last_hidden_state.shape[-1]
            tmp = encoder_outputs[1].last_hidden_state.view(batch_size, max_num_knowl, max_knowl_len, hidden_size)
            encoder_outputs[0]["last_hidden_state"] = encoder_outputs[0].last_hidden_state.index_select(0, expanded_batch_idxs)
            encoder_outputs[1]["last_hidden_state"] = tmp.index_select(0, expanded_batch_idxs).view(-1, max_knowl_len, hidden_size)

            # save encoder_outputs in `model_kwargs`
            model_kwargs["encoder_outputs"] = encoder_outputs

        else:
            cur_len = input_ids.shape[-1]

        assert (
                cur_len < max_length
        ), f"The context has {cur_len} number of tokens, but `max_length` is only {max_length}. Please make sure that `max_length` is bigger than the number of tokens, by setting either `generate(max_length=...,...)` or `config.max_length = ...`"

        if num_beams > 1:
            output = self._generate_beam_search(
                input_ids,
                cur_len=cur_len,
                max_length=max_length,
                min_length=min_length,
                do_sample=do_sample,
                early_stopping=early_stopping,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                no_repeat_ngram_size=no_repeat_ngram_size,
                bad_words_ids=bad_words_ids,
                pad_token_id=pad_token_id,
                eos_token_id=eos_token_id,
                batch_size=effective_batch_size,
                num_return_sequences=num_return_sequences,
                length_penalty=length_penalty,
                num_beams=num_beams,
                vocab_size=vocab_size,
                attention_mask=attention_mask,
                use_cache=use_cache,
                model_kwargs=model_kwargs,
            )
        else:
            output = self._generate_no_beam_search(
                input_ids,
                cur_len=cur_len,
                max_length=max_length,
                min_length=min_length,
                do_sample=do_sample,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                no_repeat_ngram_size=no_repeat_ngram_size,
                bad_words_ids=bad_words_ids,
                pad_token_id=pad_token_id,
                eos_token_id=eos_token_id,
                batch_size=effective_batch_size,
                attention_mask=attention_mask,
                use_cache=use_cache,
                model_kwargs=model_kwargs,
            )

        return output
